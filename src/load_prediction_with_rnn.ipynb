{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, mean_absolute_error, mean_squared_log_error, median_absolute_error, r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MASE(training_series, testing_series, prediction_series, m=365):\n",
    "    n = training_series.shape[0]\n",
    "    d = np.abs(  np.diff( training_series) ).sum()/(n-1)\n",
    "#     i=0\n",
    "#     d = 0\n",
    "#     while (i+m) < n:\n",
    "#         d += np.abs(training_series[i]-training_series[i+m])\n",
    "#         i += 1\n",
    "#     d = d/(n-m)\n",
    "    d1 = np.abs(training_series).mean() \n",
    "    errors = np.abs(testing_series - prediction_series)\n",
    "    return errors.mean()/d\n",
    "#     return errors.mean()/d1\n",
    "\n",
    "def MAPE(y_true, y_pred):\n",
    "    mape = 100*sum(np.divide(np.abs(y_true-y_pred),y_true))/len(y_true)\n",
    "    return mape[0]\n",
    "\n",
    "def next_batch(training_data,batch_size,steps,start_point):\n",
    "    # Grab a random starting point for each batch\n",
    "    # rand_start = np.random.randint(0,len(training_data)-steps*2) \n",
    "    rand_start = start_point\n",
    "    # Create Y data for time series in the batches\n",
    "    y_batch = np.array(training_data[rand_start:rand_start+steps+1]).reshape(1,steps+1)\n",
    "    X_batch = np.array(training_data[rand_start:rand_start+steps]).reshape(-1, steps, 1)\n",
    "    y_batch = np.array(training_data[rand_start+steps:rand_start+steps*2]).reshape(-1, steps, 1)\n",
    "    start_point += steps\n",
    "    if (start_point + 2*steps) >= len(training_data):\n",
    "        start_point = 0\n",
    "    return X_batch, y_batch, start_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  train errors:\tcoast\tRMSE: 964.56 \tMAE: 754.941 \tMASE: 1.3737 \tMAPE: 6.9901\n",
      "1000  test  errors:\tcoast\tRMSE: 1272.8966 \tMAE: 995.0476 \tMASE: 1.8106 \tMAPE: 8.1972\n",
      "1000  train errors:\teast\tRMSE: 148.9322 \tMAE: 115.5939 \tMASE: 1.3297 \tMAPE: 8.2951\n",
      "1000  test  errors:\teast\tRMSE: 193.1373 \tMAE: 144.7921 \tMASE: 1.6655 \tMAPE: 9.0942\n",
      "1000  train errors:\tercot\tRMSE: 3637.0805 \tMAE: 2812.4913 \tMASE: 1.5466 \tMAPE: 7.2611\n",
      "1000  test  errors:\tercot\tRMSE: 5050.4834 \tMAE: 3772.263 \tMASE: 2.0744 \tMAPE: 8.2104\n"
     ]
    }
   ],
   "source": [
    "datasetnames=[\"coast\",\"east\",\"ercot\",\"far_west\",\"north\",\"north_c\",\"south_c\",\"southern\",\"west\"]\n",
    "\n",
    "#datasetnames=[\"coast\"]\n",
    "\n",
    "for dtindx in range(len(datasetnames)):\n",
    "\n",
    "    datasetName = datasetnames[dtindx]\n",
    "    train = pd.read_csv('../data/Daily_Load_Data/train/'+datasetName+'.csv',index_col='Time')\n",
    "    test = pd.read_csv('../data/Daily_Load_Data/test/'+datasetName+'.csv',index_col='Time')\n",
    "    train = train.dropna(axis=0)\n",
    "    test = test.dropna(axis=0)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "\n",
    "    # Just one feature, the time series\n",
    "    num_inputs = 1\n",
    "    # Num of steps in each batch\n",
    "    num_time_steps = 365\n",
    "    # 100 neuron layer, play with this\n",
    "    num_neurons = 1500\n",
    "    # number of layer\n",
    "    num_layers = 3\n",
    "    # Just one output, predicted time series\n",
    "    num_outputs = 1\n",
    "    ## You can also try increasing iterations, but decreasing learning rate\n",
    "    # learning rate you can play with this\n",
    "    learning_rate = 0.001\n",
    "    # how many iterations to go through (training steps), you can play with this\n",
    "    num_train_iterations = 1000\n",
    "    # Size of the batch of data\n",
    "    batch_size = 1\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\n",
    "    y = tf.placeholder(tf.float32, [None, num_time_steps, num_outputs])\n",
    "\n",
    "    cell = tf.contrib.rnn.OutputProjectionWrapper(tf.nn.rnn_cell.MultiRNNCell(\n",
    "        [tf.nn.rnn_cell.GRUCell(num_units=num_neurons, activation=tf.nn.relu) \n",
    "                                        for layer in range(num_layers)]),\n",
    "                                        output_size=num_outputs)\n",
    "#     cell = tf.contrib.rnn.OutputProjectionWrapper(tf.nn.rnn_cell.MultiRNNCell(\n",
    "#         [tf.nn.rnn_cell.LSTMCell(num_units=num_neurons, activation=tf.nn.relu) \n",
    "#                                         for layer in range(num_layers)]),\n",
    "#                                         output_size=num_outputs)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    loss = tf.reduce_mean(tf.square(outputs - y)) # MSE\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train = optimizer.minimize(loss)\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "    model_file_name = \"../../temp/model_\"+datasetName+\".ckpt\"\n",
    "    saver = tf.train.Saver()\n",
    "    start_point = 0\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(init)\n",
    "#         saver.restore(sess, model_file_name)\n",
    "        for iteration in range(num_train_iterations+1):\n",
    "            X_batch, y_batch, start_point = next_batch(train_scaled,batch_size,num_time_steps,start_point)\n",
    "            sess.run(train, feed_dict={X: X_batch, y: y_batch})\n",
    "            if iteration % 1000 == 0 and iteration != 0:\n",
    "                try:\n",
    "                    #mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    #print(iteration, \"\\tMSE:\", mse)\n",
    "                    \n",
    "                    # calculate train error\n",
    "                    s_point = 0\n",
    "                    mse = 0\n",
    "                    mae = 0\n",
    "                    mase = 0\n",
    "                    mape = 0\n",
    "                    trun = 10\n",
    "                    for i in range(trun):\n",
    "                        X_batch, y_batch, s_point = next_batch(train_scaled,batch_size,num_time_steps,s_point)\n",
    "                        y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "                        results = scaler.inverse_transform(y_pred.reshape(num_time_steps,1))\n",
    "                        true_data = y_batch.reshape(num_time_steps,1)\n",
    "                        true_data = scaler.inverse_transform(true_data)\n",
    "                        n = min(results.shape[0],true_data.shape[0])\n",
    "\n",
    "                        y_pred = results[:n]\n",
    "                        y_true = true_data[:n]\n",
    "\n",
    "                        mse += mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "                        mae += mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "\n",
    "                        train_series = scaler.inverse_transform(train_scaled)\n",
    "                        mase += MASE(train_series.ravel(),y_true.ravel(),y_pred.ravel())\n",
    "                        mape += MAPE(y_true, y_pred)\n",
    "                    mse /= trun\n",
    "                    mae /= trun \n",
    "                    mase /= trun\n",
    "                    mape /= trun\n",
    "                    print(iteration,\" train errors:\\t\"+datasetName+'\\tRMSE:', round(mse**0.5,4), '\\tMAE:', \n",
    "                          round(mae,4), '\\tMASE:', round(mase,4), '\\tMAPE:', round(mape,4))\n",
    "                    \n",
    "                    #test error\n",
    "                    train_seed = list(train_scaled[-num_time_steps:])\n",
    "                    X_batch = np.array(train_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n",
    "                    y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "                    results = scaler.inverse_transform(y_pred.reshape(num_time_steps,1))\n",
    "                    true_data = test.values\n",
    "                    n = min(results.shape[0],true_data.shape[0])\n",
    "\n",
    "                    y_pred = results[:n]\n",
    "                    y_true = true_data[:n]\n",
    "\n",
    "                    mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "                    mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "\n",
    "                    train_series = scaler.inverse_transform(train_scaled)\n",
    "                    mase = MASE(train_series.ravel(),y_true.ravel(),y_pred.ravel())\n",
    "                    mape = MAPE(y_true, y_pred)\n",
    "\n",
    "                    print(iteration,\" test  errors:\\t\"+datasetName+'\\tRMSE:', round(mse**0.5,4), '\\tMAE:', \n",
    "                          round(mae,4), '\\tMASE:', round(mase,4), '\\tMAPE:', round(mape,4))\n",
    "                    saver.save(sess, model_file_name)\n",
    "                except Exception as e:\n",
    "                    print(\"error1\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(abs(y_pred-y_true))/len(y_true)\n",
    "# plt.plot(y_true.ravel())\n",
    "# plt.plot(y_pred.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # d = np.abs(np.diff(y_true)).sum()\n",
    "# # print(d)\n",
    "# d =(np.diff(y_true.ravel()))\n",
    "# d= sum(abs(d))/len(d)\n",
    "# print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#             # calculate train error\n",
    "#             start_point = 0\n",
    "#             mse = 0\n",
    "#             mae = 0\n",
    "#             mase = 0\n",
    "#             mape = 0\n",
    "#             trun = 10\n",
    "#             for i in range(trun):\n",
    "#                 X_batch, y_batch, start_point = next_batch(train_scaled,batch_size,num_time_steps,start_point)\n",
    "#                 y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "#                 results = scaler.inverse_transform(y_pred.reshape(num_time_steps,1))\n",
    "#                 true_data = y_batch.reshape(num_time_steps,1)\n",
    "#                 true_data = scaler.inverse_transform(true_data)\n",
    "#                 n = min(results.shape[0],true_data.shape[0])\n",
    "\n",
    "#                 y_pred = results[:n]\n",
    "#                 y_true = true_data[:n]\n",
    "\n",
    "#                 mse += mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "#                 mae += mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "\n",
    "#                 train_series = scaler.inverse_transform(train_scaled)\n",
    "#                 mase += MASE(train_series.ravel(),y_true.ravel(),y_pred.ravel())\n",
    "#                 mape += MAPE(y_true, y_pred)\n",
    "#             mse /= trun\n",
    "#             mae /= trun \n",
    "#             mase /= trun\n",
    "#             mape /= trun\n",
    "#             print(\"train errors:\\t\"+datasetName+'\\tRMSE:', round(mse**0.5,4), '\\tMAE:', \n",
    "#                   round(mae,4), '\\tMASE:', round(mase,4), '\\tMAPE:', round(mape,4))\n",
    "\n",
    "#             # calculate test error\n",
    "#             train_seed = list(train_scaled[-num_time_steps:])\n",
    "#             X_batch = np.array(train_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n",
    "#             y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "#             results = scaler.inverse_transform(y_pred.reshape(num_time_steps,1))\n",
    "#             true_data = test.values\n",
    "#             n = min(results.shape[0],true_data.shape[0])\n",
    "\n",
    "#             y_pred = results[:n]\n",
    "#             y_true = true_data[:n]\n",
    "\n",
    "#             mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "#             mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "\n",
    "#             train_series = scaler.inverse_transform(train_scaled)\n",
    "#             mase = MASE(train_series.ravel(),y_true.ravel(),y_pred.ravel())\n",
    "#             mape = MAPE(y_true, y_pred)\n",
    "\n",
    "#             print('test errors:\\t'+datasetName+'\\tRMSE:', round(mse**0.5,4), '\\tMAE:', \n",
    "#                   round(mae,4), '\\tMASE:', round(mase,4), '\\tMAPE:', round(mape,4))\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(\"error2\")\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "# prediction\n",
    "    #train_seed = list(train_scaled[-num_time_steps:])\n",
    "    \n",
    "    ## Now create a for loop that \n",
    "#     for iteration in range(num_time_steps):\n",
    "#         X_batch = np.array(train_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n",
    "#         y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "#         train_seed.append(y_pred[0, -1, 0])\n",
    "#     results = scaler.inverse_transform(np.array(train_seed[num_time_steps:]).reshape(num_time_steps,1))\n",
    "    \n",
    "    #X_batch = np.array(train_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n",
    "    #y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "    #results = scaler.inverse_transform(y_pred.reshape(num_time_steps,1))\n",
    "    #true_data = test.values\n",
    "    #n = min(results.shape[0],true_data.shape[0])\n",
    "    #y_pred = results[:n]\n",
    "    #y_true = true_data[:n]\n",
    "    \n",
    "    #mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "    #print('RMSE:', mse**0.5)\n",
    "#     evs = explained_variance_score(y_true=y_true,y_pred=y_pred)\n",
    "#     print('EVS:', evs)\n",
    "    #mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "    #print('MAE:', mae)\n",
    "#     msle = mean_squared_log_error(y_true=y_true,y_pred=y_pred)\n",
    "#     print('MSLE:', msle)\n",
    "#     meae = median_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "#     print('MEAE:', meae)\n",
    "#     r2 = r2_score(y_true=y_true,y_pred=y_pred)\n",
    "#     print('R2-Score:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_truedf = pd.DataFrame(data=y_true)\n",
    "y_preddf = pd.DataFrame(data=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_truedf.to_csv(\"y_true.csv\")\n",
    "y_preddf.to_csv(\"y_pred.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASE(train_series.ravel(),y_true.ravel(),y_pred.ravel(),m=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
