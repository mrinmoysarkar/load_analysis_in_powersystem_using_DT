{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, mean_absolute_error, mean_squared_log_error, median_absolute_error, r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName = \"west\"\n",
    "train = pd.read_csv('../data/Daily_Load_Data/train/'+datasetName+'.csv',index_col='Time')\n",
    "test = pd.read_csv('../data/Daily_Load_Data/test/'+datasetName+'.csv',index_col='Time')\n",
    "train = train.dropna(axis=0)\n",
    "test = test.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MASE(training_series, testing_series, prediction_series, m=365):\n",
    "    n = training_series.shape[0]\n",
    "#     d = np.abs(  np.diff( training_series) ).sum()/(n-1)\n",
    "    i=0\n",
    "    d = 0\n",
    "    while (i+m) < n:\n",
    "        d += np.abs(training_series[i]-training_series[i+m])\n",
    "        i += 1\n",
    "    d = d/(n-m)\n",
    "    errors = np.abs(testing_series - prediction_series )\n",
    "    return errors.mean()/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred):\n",
    "    mape = 100*sum(np.divide(np.abs(y_true-y_pred),y_true))/len(y_true)\n",
    "    return mape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(training_data,batch_size,steps,start_point):\n",
    "    # Grab a random starting point for each batch\n",
    "    # rand_start = np.random.randint(0,len(training_data)-steps*2) \n",
    "    rand_start = start_point\n",
    "    # Create Y data for time series in the batches\n",
    "    y_batch = np.array(training_data[rand_start:rand_start+steps+1]).reshape(1,steps+1)\n",
    "    X_batch = np.array(training_data[rand_start:rand_start+steps]).reshape(-1, steps, 1)\n",
    "    y_batch = np.array(training_data[rand_start+steps:rand_start+steps*2]).reshape(-1, steps, 1)\n",
    "    start_point += steps\n",
    "    if start_point + 2*steps >= len(training_data):\n",
    "        start_point = 0\n",
    "    return X_batch, y_batch, start_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just one feature, the time series\n",
    "num_inputs = 1\n",
    "# Num of steps in each batch\n",
    "num_time_steps = 365\n",
    "# 100 neuron layer, play with this\n",
    "num_neurons = 2000\n",
    "# number of layer\n",
    "num_layers = 3\n",
    "# Just one output, predicted time series\n",
    "num_outputs = 1\n",
    "## You can also try increasing iterations, but decreasing learning rate\n",
    "# learning rate you can play with this\n",
    "learning_rate = 0.001\n",
    "# how many iterations to go through (training steps), you can play with this\n",
    "num_train_iterations = 10000\n",
    "# Size of the batch of data\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, num_time_steps, num_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, num_time_steps, num_outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.OutputProjectionWrapper(tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.GRUCell(num_units=num_neurons, activation=tf.nn.relu) \n",
    "                                    for layer in range(num_layers)]),\n",
    "                                    output_size=num_outputs)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "loss = tf.reduce_mean(tf.square(outputs - y)) # MSE\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_file_name = \"./tmp/model_\"+datasetName+\".ckpt\"\n",
    "saver = tf.train.Saver()\n",
    "start_point = 0\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "    sess.run(init)\n",
    "    for iteration in range(num_train_iterations+1):\n",
    "        X_batch, y_batch, start_point = next_batch(train_scaled,batch_size,num_time_steps,start_point)\n",
    "        sess.run(train, feed_dict={X: X_batch, y: y_batch})\n",
    "        if iteration % 1000 == 0:\n",
    "            try:\n",
    "                #mse = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                #print(iteration, \"\\tMSE:\", mse)\n",
    "                train_seed = list(train_scaled[-num_time_steps:])\n",
    "                X_batch = np.array(train_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n",
    "                y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "                results = scaler.inverse_transform(y_pred.reshape(num_time_steps,1))\n",
    "                true_data = test.values\n",
    "                n = min(results.shape[0],true_data.shape[0])\n",
    "\n",
    "                y_pred = results[:n]\n",
    "                y_true = true_data[:n]\n",
    "\n",
    "                mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "                mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "\n",
    "                train_series = scaler.inverse_transform(train_scaled)\n",
    "                mase = MASE(train_series.ravel(),y_true.ravel(),y_pred.ravel())\n",
    "                mape = MAPE(y_true, y_pred)\n",
    "\n",
    "                print(iteration,'\\t'+datasetName+'\\tRMSE:', mse**0.5, '\\tMAE:', mae, '\\tMASE:', mase, '\\tMAPE:', mape)\n",
    "                saver.save(sess, model_file_name)\n",
    "            except Exception as e:\n",
    "                print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # prediction\n",
    "    #train_seed = list(train_scaled[-num_time_steps:])\n",
    "    \n",
    "    ## Now create a for loop that \n",
    "#     for iteration in range(num_time_steps):\n",
    "#         X_batch = np.array(train_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n",
    "#         y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "#         train_seed.append(y_pred[0, -1, 0])\n",
    "#     results = scaler.inverse_transform(np.array(train_seed[num_time_steps:]).reshape(num_time_steps,1))\n",
    "    \n",
    "    #X_batch = np.array(train_seed[-num_time_steps:]).reshape(1, num_time_steps, 1)\n",
    "    #y_pred = sess.run(outputs, feed_dict={X: X_batch})\n",
    "    #results = scaler.inverse_transform(y_pred.reshape(num_time_steps,1))\n",
    "    #true_data = test.values\n",
    "    #n = min(results.shape[0],true_data.shape[0])\n",
    "    #y_pred = results[:n]\n",
    "    #y_true = true_data[:n]\n",
    "    \n",
    "    #mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "    #print('RMSE:', mse**0.5)\n",
    "#     evs = explained_variance_score(y_true=y_true,y_pred=y_pred)\n",
    "#     print('EVS:', evs)\n",
    "    #mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "    #print('MAE:', mae)\n",
    "#     msle = mean_squared_log_error(y_true=y_true,y_pred=y_pred)\n",
    "#     print('MSLE:', msle)\n",
    "#     meae = median_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "#     print('MEAE:', meae)\n",
    "#     r2 = r2_score(y_true=y_true,y_pred=y_pred)\n",
    "#     print('R2-Score:', r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
